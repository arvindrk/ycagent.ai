# Semantic Search Architecture - YC Agent

**Last Updated:** January 24, 2026  
**Status:** Phase 1 Complete ‚úÖ | Phase 2 In Progress üöß  
**Target:** Production-ready semantic search for company discovery

---

## Table of Contents

1. [Overview](#overview)
2. [Architecture Design](#architecture-design)
3. [Technology Stack](#technology-stack)
4. [Database Schema](#database-schema)
5. [Implementation Phases](#implementation-phases)
6. [Code Structure](#code-structure)
7. [Testing Strategy](#testing-strategy)
8. [Performance & Optimization](#performance--optimization)
9. [Cost Analysis](#cost-analysis)
10. [Rollout Plan](#rollout-plan)

---

## Overview

### Problem Statement

Users need to search across 10,000+ YC companies using natural language queries that span multiple dimensions:

- **Semantic content**: "companies building eval tooling for LLMs"
- **Geographic filters**: "in San Francisco" or "remote"
- **Metadata filters**: "W24 batch", "small teams", "hiring"
- **Industry/tags**: "B2B infrastructure"

Traditional keyword search cannot understand semantic meaning. Users searching for "LLM evaluation tools" won't find companies describing themselves as "AI model testing platforms."

### Solution

Implement **pgvector-powered semantic search** using OpenAI embeddings to understand query intent and match against company descriptions, combined with structured SQL filters for metadata.

### Core Principle

**Keep It Simple.** Start with semantic search for all multi-word queries. Optimize later based on real usage patterns.

---

## Architecture Design

### High-Level Flow

```
User Query
    ‚Üì
Parse Query & Filters
    ‚Üì
Generate Query Embedding (OpenAI API)
    ‚Üì
Vector Similarity Search (pgvector HNSW index)
    +
Structured Filters (SQL WHERE clauses)
    ‚Üì
Score & Rank Results
    ‚Üì
Return Top N Companies
```

### Search Strategy

**Single-tier semantic search:**

- All queries use vector similarity search
- Structured filters applied as SQL WHERE clauses
- Hybrid scoring: 70% semantic + 20% name fuzzy + 10% full-text

**Why not tiered?**

- Sample queries all require semantic understanding
- Embeddings are cheap ($0.0001/query)
- Simpler code = fewer bugs
- Can add optimization later if needed

---

## Technology Stack

### Core Technologies

| Component            | Technology                    | Purpose                              |
| -------------------- | ----------------------------- | ------------------------------------ |
| **Database**         | PostgreSQL (Neon)             | Primary data store                   |
| **Vector Extension** | pgvector 0.8.0+               | Vector similarity search             |
| **Embeddings**       | OpenAI text-embedding-3-small | Generate 1536-dim vectors            |
| **Index Type**       | HNSW                          | Approximate nearest neighbor         |
| **ORM/Client**       | @neondatabase/serverless      | Serverless-optimized Postgres driver |

### Why These Choices?

- **pgvector**: Native Postgres extension, production-ready, ACID compliant
- **HNSW index**: Better query performance than IVFFlat, suitable for 10k dataset
- **text-embedding-3-small**: Cost-effective ($0.13/1M tokens), 768 dimensions (reduced for efficiency), good quality
- **Neon**: Built-in pgvector support, serverless scaling, generous free tier

---

## Database Schema

### Current Schema (Keep As-Is)

```sql
CREATE TABLE companies (
    -- Core fields
    id UUID PRIMARY KEY,
    name TEXT NOT NULL,
    one_liner TEXT,
    long_description TEXT,

    -- JSONB arrays (KEEP - works great with GIN indexes)
    tags JSONB DEFAULT '[]'::jsonb,
    industries JSONB DEFAULT '[]'::jsonb,
    regions JSONB DEFAULT '[]'::jsonb,

    -- Filters
    batch VARCHAR(50),
    team_size INTEGER,
    stage VARCHAR(50),
    is_hiring BOOLEAN,
    all_locations TEXT,

    -- Existing indexes
    -- idx_companies_tags GIN(tags jsonb_path_ops)
    -- idx_companies_name_trgm GIN(name gin_trgm_ops)
);
```

### New Schema Additions

```sql
-- Migration: 002_add_vector_search.sql

-- 1. Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- 2. Add vector column for embeddings (768 dimensions)
ALTER TABLE companies ADD COLUMN embedding vector(768);

-- 3. Add full-text search support (lightweight fallback)
ALTER TABLE companies ADD COLUMN search_vector tsvector
  GENERATED ALWAYS AS (
    setweight(to_tsvector('english', name), 'A') ||
    setweight(to_tsvector('english', COALESCE(one_liner, '')), 'B') ||
    setweight(to_tsvector('english', COALESCE(long_description, '')), 'C') ||
    setweight(to_tsvector('english', COALESCE(all_locations, '')), 'D')
  ) STORED;

-- 4. Create HNSW index for vector similarity
CREATE INDEX idx_companies_embedding_hnsw
  ON companies USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);

-- 5. Create GIN index for full-text search
CREATE INDEX idx_companies_search_vector
  ON companies USING GIN(search_vector);

-- 6. Add missing filter indexes

CREATE INDEX idx_companies_batch
  ON companies(batch) WHERE batch IS NOT NULL;
```

### Storage Impact

| Component           | Size per Row              | Total (5.7k companies) |
| ------------------- | ------------------------- | ---------------------- |
| **Vector column**   | ~3KB (768 dims √ó 4 bytes) | ~17MB                  |
| **HNSW index**      | ~2-3√ó vector size         | ~34-51MB               |
| **Full-text index** | ~1-2KB                    | ~6-11MB                |
| **Total added**     | ~5-6KB                    | **~57-79MB**           |

### Index Strategy

**Philosophy:** After vector search narrows results to ~100 candidates, sequential scans for filtering are fast enough. Only index what's proven slow through production metrics.

---

## Implementation Phases

### Phase 1: Database Setup ‚úÖ **COMPLETE**

**Status:** ‚úÖ Completed January 24, 2026

**Files Created:**

- ‚úÖ `db/migrations/002_add_vector_search.sql`
- ‚úÖ `scripts/verify-phase1.ts`

**Results:**

- ‚úÖ `embedding` column exists with type `vector(768)`
- ‚úÖ HNSW index created (m=16, ef_construction=64)
- ‚úÖ Full-text search index created
- ‚úÖ Filter indexes created (batch, stage, team_size)
- ‚úÖ JSONB queries verified (776 AI/ML companies, 2,852 B2B/SaaS companies)
- ‚úÖ Data integrity preserved (5,653 companies intact)
- ‚úÖ 11 total indexes on companies table

**Verification Command:**

```bash
npx dotenvx run -- tsx scripts/verify-phase1.ts
```

---

### Phase 2: Embedding Generation (1-2 hours) üöß **IN PROGRESS**

**Status:** üöß Implementation in progress

**Architecture Decision:** Decoupled, vendor-agnostic design for easy provider swapping

**Design Principles:**

- **Pure functions** for text preparation (no vendor dependencies)
- **Provider interface** for vendor abstraction
- **Factory pattern** for easy provider switching
- **Tight coupling in bulk script** (acceptable for one-time use)

**Files to Create:**

- `src/lib/embeddings/types.ts` - Vendor-agnostic interfaces
- `src/lib/embeddings/text-preparation.ts` - Pure text formatting functions
- `src/lib/embeddings/providers/openai.ts` - OpenAI implementation
- `src/lib/embeddings/providers/index.ts` - Provider factory
- `src/lib/embeddings/index.ts` - Public API (barrel export)
- `scripts/generate-embeddings-bulk.ts` - One-time bulk processor (OpenAI-coupled)

**Implementation:**

#### `src/lib/embeddings/types.ts` (Vendor-Agnostic)

```typescript
/**
 * Vendor-agnostic embedding provider interface
 * Allows easy swapping between OpenAI, Cohere, Anthropic, local models, etc.
 */
export interface EmbeddingProvider {
  name: string;
  dimensions: number;
  generate(text: string): Promise<number[]>;
  generateBatch(texts: string[]): Promise<number[][]>;
}

export interface EmbeddingConfig {
  provider: 'openai' | 'cohere' | 'anthropic' | 'local';
  dimensions: 768;
  model?: string;
  apiKey?: string;
}

export interface EmbeddingResult {
  embedding: number[];
  dimensions: number;
  provider: string;
}

export interface Company {
  name: string;
  one_liner: string | null;
  long_description: string | null;
  tags: string[];
  industries: string[];
  all_locations: string | null;
  batch: string | null;
}
```

#### `src/lib/embeddings/text-preparation.ts` (Pure Functions)

```typescript
import type { Company } from './types';

/**
 * Pure function: Create embedding text from company data
 * No vendor dependencies - can be used with any embedding provider
 * Prioritizes description content over metadata
 */
export function createEmbeddingText(company: Company): string {
  const parts = [
    company.name,
    company.one_liner,
    company.long_description,
    company.tags?.length > 0
      ? `Categories: ${company.tags.join(', ')}`
      : null,
    company.industries?.length > 0
      ? `Industries: ${company.industries.join(', ')}`
      : null,
    company.all_locations ? `Location: ${company.all_locations}` : null,
    company.batch ? `YC ${company.batch}` : null,
  ];

  return parts.filter(Boolean).join('. ');
}

/**
 * Pure function: Prepare company for embedding
 * Returns structured data ready for embedding
 */
export function prepareCompanyForEmbedding(company: Company) {
  return {
    id: company.id,
    text: createEmbeddingText(company),
  };
}
```

#### `src/lib/embeddings/providers/openai.ts` (OpenAI Implementation)

```typescript
import OpenAI from 'openai';
import type { EmbeddingProvider, EmbeddingConfig } from '../types';

export class OpenAIEmbeddingProvider implements EmbeddingProvider {
  name = 'openai';
  dimensions = 768;
  private client: OpenAI;
  private model: string;

  constructor(config: EmbeddingConfig) {
    this.client = new OpenAI({
      apiKey: config.apiKey || process.env.OPENAI_API_KEY,
    });
    this.model = config.model || 'text-embedding-3-small';
    this.dimensions = config.dimensions;
  }

  async generate(text: string): Promise<number[]> {
    const response = await this.client.embeddings.create({
      model: this.model,
      input: text,
      dimensions: this.dimensions,
    });

    return response.data[0].embedding;
  }

  async generateBatch(texts: string[]): Promise<number[][]> {
    if (texts.length > 2048) {
      throw new Error('OpenAI batch size cannot exceed 2048');
    }

    const response = await this.client.embeddings.create({
      model: this.model,
      input: texts,
      dimensions: this.dimensions,
    });

    return response.data.map((item) => item.embedding);
  }
}
```

#### `src/lib/embeddings/providers/index.ts` (Factory)

```typescript
import type { EmbeddingProvider, EmbeddingConfig } from '../types';
import { OpenAIEmbeddingProvider } from './openai';

/**
 * Factory function to get embedding provider
 * Makes it easy to swap providers by changing config
 */
export function getEmbeddingProvider(
  config: EmbeddingConfig
): EmbeddingProvider {
  switch (config.provider) {
    case 'openai':
      return new OpenAIEmbeddingProvider(config);
    // Future providers:
    // case 'cohere': return new CohereEmbeddingProvider(config);
    // case 'anthropic': return new AnthropicEmbeddingProvider(config);
    // case 'local': return new LocalModelProvider(config);
    default:
      throw new Error(`Unknown embedding provider: ${config.provider}`);
  }
}
```

#### `src/lib/embeddings/index.ts` (Public API)

```typescript
// Re-export public API
export * from './types';
export * from './text-preparation';
export * from './providers';
```

#### `scripts/generate-embeddings-bulk.ts` (Tightly Coupled to OpenAI)

**Note:** This script is intentionally tightly coupled to OpenAI SDK for simplicity. It's a one-time operation, so the coupling is acceptable. The reusable embedding logic is decoupled in `src/lib/embeddings/`.

```typescript
import { neon } from '@neondatabase/serverless';
import OpenAI from 'openai';
import { createEmbeddingText } from '../src/lib/embeddings/text-preparation';

const sql = neon(process.env.DATABASE_URL!);
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

interface Company {
  id: string;
  name: string;
  one_liner: string | null;
  long_description: string | null;
  tags: string[];
  industries: string[];
  all_locations: string | null;
  batch: string | null;
}

async function main() {
  console.log('üöÄ Starting embedding generation...\n');

  // Fetch companies without embeddings
  const companies = await sql<Company[]>`
    SELECT id, name, one_liner, long_description, tags, 
           industries, all_locations, batch
    FROM companies 
    WHERE embedding IS NULL
    ORDER BY created_at DESC
  `;

  if (companies.length === 0) {
    console.log('‚úÖ All companies already have embeddings');
    return;
  }

  console.log(`Found ${companies.length} companies to process\n`);

  const BATCH_SIZE = 100;
  let processed = 0;
  let errors = 0;

  for (let i = 0; i < companies.length; i += BATCH_SIZE) {
    const batch = companies.slice(i, i + BATCH_SIZE);
    const batchNum = Math.floor(i / BATCH_SIZE) + 1;
    const totalBatches = Math.ceil(companies.length / BATCH_SIZE);

    try {
      // Use pure function for text preparation (vendor-agnostic)
      const texts = batch.map(createEmbeddingText);

      // Direct OpenAI API call (tight coupling acceptable here)
      const response = await openai.embeddings.create({
        model: 'text-embedding-3-small',
        input: texts,
        dimensions: 768,
      });

      const embeddings = response.data.map((item) => item.embedding);

      // Update database in parallel
      await Promise.all(
        batch.map((company, idx) =>
          sql`
            UPDATE companies 
            SET embedding = ${JSON.stringify(embeddings[idx])}::vector
            WHERE id = ${company.id}
          `
        )
      );

      processed += batch.length;
      const progress = ((processed / companies.length) * 100).toFixed(1);
      console.log(
        `‚úì Batch ${batchNum}/${totalBatches} complete (${progress}%)`
      );
      console.log(`  Processed: ${batch[0].name}, ${batch[1]?.name || ''}...`);

      // Rate limiting: OpenAI has 3000 RPM limit
      // Sleep 1s between batches to stay under limit
      if (i + BATCH_SIZE < companies.length) {
        await new Promise((resolve) => setTimeout(resolve, 1000));
      }
    } catch (error) {
      console.error(`‚ùå Error processing batch ${batchNum}:`, error);
      errors++;
      continue;
    }
  }

  console.log('\n' + '='.repeat(50));
  console.log(`‚úÖ Embedding generation complete!`);
  console.log(`   Total processed: ${processed}/${companies.length}`);
  if (errors > 0) {
    console.log(`   ‚ö†Ô∏è  Errors: ${errors} batches failed`);
  }
  console.log('='.repeat(50));
}

main().catch(console.error);
```

**Commands:**

```bash
# Install dependencies (already done)
npm install openai

# Add to .env (already done)
# OPENAI_API_KEY=sk-...

# Run generation
npx dotenvx run -- tsx scripts/generate-embeddings-bulk.ts
```

**Expected Output:**

```
üöÄ Starting embedding generation...

Found 5653 companies to process

‚úì Batch 1/57 complete (1.8%)
  Processed: Stripe, OpenAI...
‚úì Batch 2/57 complete (3.5%)
  Processed: Anthropic, Vercel...
...
‚úÖ Embedding generation complete!
   Total processed: 5653/5653
```

**Success Criteria:**

- ‚úÖ All companies have embeddings
- ‚úÖ No NULL embeddings remain
- ‚úÖ Embeddings are 768 dimensions
- ‚úÖ Total cost < $2
- ‚úÖ Decoupled architecture allows easy provider swapping

---

### Phase 3: Search Query Implementation (2 hours)

**Design Principles:**

- ‚úÖ Pure functions for testability
- ‚úÖ Modular architecture with clear separation of concerns
- ‚úÖ Optimize for accuracy over speed
- ‚úÖ Follow React/Next.js best practices (async-parallel, server patterns)

**Architecture:**

```
src/lib/search/
‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îî‚îÄ‚îÄ generate.ts          # Pure: text ‚Üí embedding
‚îú‚îÄ‚îÄ filters/
‚îÇ   ‚îú‚îÄ‚îÄ parse.ts             # Pure: raw params ‚Üí typed filters
‚îÇ   ‚îî‚îÄ‚îÄ build.ts             # Pure: filters ‚Üí SQL clauses
‚îú‚îÄ‚îÄ scoring/
‚îÇ   ‚îî‚îÄ‚îÄ weights.ts           # Pure: scoring configuration
‚îî‚îÄ‚îÄ query.ts                 # Orchestration (only impure part)
```

**Files to Create:**

- `src/lib/search/embeddings/generate.ts` - Embedding utilities
- `src/lib/search/filters/parse.ts` - Filter parsing
- `src/lib/search/filters/build.ts` - SQL clause generation
- `src/lib/search/scoring/weights.ts` - Scoring configuration
- `src/lib/search/query.ts` - Main search orchestration
- `src/app/api/search/route.ts` - API endpoint
- `src/lib/validations/search.schema.ts` - Input validation

**Implementation:**

#### `src/lib/search/scoring/weights.ts`

```typescript
/**
 * Scoring weights for hybrid search
 * Pure configuration - easy to tune for accuracy
 */

export const SEARCH_SCORING = {
    // Optimize for semantic accuracy
    SEMANTIC_WEIGHT: 0.8, // 80% - semantic understanding
    NAME_WEIGHT: 0.15, // 15% - name fuzzy matching
    FULLTEXT_WEIGHT: 0.05, // 5% - keyword fallback

    // Minimum score thresholds
    MIN_SEMANTIC_SCORE: 0.1,
    MIN_NAME_SCORE: 0.3,
    MIN_TOTAL_SCORE: 0.05,
} as const;

export const HNSW_CONFIG = {
    // Higher ef_search = better recall (accuracy over speed)
    EF_SEARCH: 200, // Default: 40, Range: 10-400
} as const;

/**
 * Calculate weighted final score
 */
export function calculateFinalScore(
    semanticScore: number,
    nameScore: number,
    textScore: number
): number {
    return (
        semanticScore * SEARCH_SCORING.SEMANTIC_WEIGHT +
        nameScore * SEARCH_SCORING.NAME_WEIGHT +
        textScore * SEARCH_SCORING.FULLTEXT_WEIGHT
    );
}

/**
 * Check if result meets minimum quality threshold
 */
export function meetsMinimumQuality(
    semanticScore: number,
    nameScore: number,
    textScore: number
): boolean {
    const finalScore = calculateFinalScore(semanticScore, nameScore, textScore);

    return (
        finalScore >= SEARCH_SCORING.MIN_TOTAL_SCORE &&
        (semanticScore >= SEARCH_SCORING.MIN_SEMANTIC_SCORE ||
            nameScore >= SEARCH_SCORING.MIN_NAME_SCORE ||
            textScore > 0)
    );
}
```

#### `src/lib/search/filters/parse.ts`

```typescript
/**
 * Pure functions for parsing search filters
 * Separates parsing logic from validation and SQL generation
 */

export interface RawSearchParams {
    q: string;
    batch?: string;
    stage?: string;
    status?: string;
    tags?: string;
    industries?: string;
    regions?: string;
    team_size_min?: string;
    team_size_max?: string;
    is_hiring?: string;
    is_nonprofit?: string;
    location?: string;
    limit?: string;
    offset?: string;
}

export interface ParsedFilters {
    // Exact matches
    batch?: string;
    stage?: string;
    status?: string;

    // Array filters
    tags?: string[];
    industries?: string[];
    regions?: string[];

    // Numeric ranges
    team_size_min?: number;
    team_size_max?: number;

    // Boolean filters
    is_hiring?: boolean;
    is_nonprofit?: boolean;

    // Fuzzy text
    location?: string;
}

/**
 * Parse comma-separated string into array
 */
function parseArrayFilter(value: string | undefined): string[] | undefined {
    if (!value) return undefined;
    return value
        .split(',')
        .map((v) => v.trim())
        .filter(Boolean);
}

/**
 * Parse boolean from string
 */
function parseBooleanFilter(value: string | undefined): boolean | undefined {
    if (value === 'true') return true;
    if (value === 'false') return false;
    return undefined;
}

/**
 * Parse filters from raw params (pure function)
 */
export function parseSearchFilters(params: RawSearchParams): ParsedFilters {
    return {
        batch: params.batch,
        stage: params.stage,
        status: params.status,
        tags: parseArrayFilter(params.tags),
        industries: parseArrayFilter(params.industries),
        regions: parseArrayFilter(params.regions),
        team_size_min: params.team_size_min
            ? Number(params.team_size_min)
            : undefined,
        team_size_max: params.team_size_max
            ? Number(params.team_size_max)
            : undefined,
        is_hiring: parseBooleanFilter(params.is_hiring),
        is_nonprofit: parseBooleanFilter(params.is_nonprofit),
        location: params.location,
    };
}
```

#### `src/lib/search/filters/build.ts`

```typescript
import { neon } from '@neondatabase/serverless';
import type { ParsedFilters } from './parse';

type SQLFragment = ReturnType<typeof neon.sql>;

/**
 * Build SQL WHERE clauses from parsed filters (pure-ish function)
 * Returns array of SQL fragments
 */
export function buildFilterClauses(
    sql: typeof neon.sql,
    filters: ParsedFilters
): SQLFragment[] {
    const clauses: SQLFragment[] = [];

    // String equality filters
    if (filters.batch) {
        clauses.push(sql`batch = ${filters.batch}`);
    }

    if (filters.stage) {
        clauses.push(sql`stage = ${filters.stage}`);
    }

    if (filters.status) {
        clauses.push(sql`status = ${filters.status}`);
    }

    // JSONB array filters (use ?| operator)
    if (filters.tags && filters.tags.length > 0) {
        clauses.push(sql`tags ?| ${filters.tags}`);
    }

    if (filters.industries && filters.industries.length > 0) {
        clauses.push(sql`industries ?| ${filters.industries}`);
    }

    if (filters.regions && filters.regions.length > 0) {
        clauses.push(sql`regions ?| ${filters.regions}`);
    }

    // Numeric range filters
    if (filters.team_size_min !== undefined) {
        clauses.push(sql`team_size >= ${filters.team_size_min}`);
    }

    if (filters.team_size_max !== undefined) {
        clauses.push(sql`team_size <= ${filters.team_size_max}`);
    }

    // Boolean filters
    if (filters.is_hiring !== undefined) {
        clauses.push(sql`is_hiring = ${filters.is_hiring}`);
    }

    if (filters.is_nonprofit !== undefined) {
        clauses.push(sql`is_nonprofit = ${filters.is_nonprofit}`);
    }

    // Text search with ILIKE
    if (filters.location) {
        clauses.push(sql`all_locations ILIKE ${'%' + filters.location + '%'}`);
    }

    return clauses;
}

/**
 * Combine filter clauses into WHERE clause
 */
export function buildWhereClause(
    sql: typeof neon.sql,
    clauses: SQLFragment[]
): SQLFragment {
    if (clauses.length === 0) {
        return sql`WHERE embedding IS NOT NULL`;
    }

    return sql`WHERE ${sql.join(clauses, sql` AND `)} AND embedding IS NOT NULL`;
}
```

#### `src/lib/validations/search.schema.ts`

```typescript
import { z } from 'zod';

export const searchInputSchema = z.object({
    q: z.string().min(1).max(500),

    // Exact matches
    batch: z.string().optional(),
    stage: z.string().optional(),
    status: z.string().optional(),

    // Array filters (comma-separated strings)
    tags: z.string().optional(),
    industries: z.string().optional(),
    regions: z.string().optional(),

    // Numeric ranges
    team_size_min: z.coerce.number().int().min(1).optional(),
    team_size_max: z.coerce.number().int().min(1).optional(),

    // Boolean filters
    is_hiring: z.enum(['true', 'false']).optional(),
    is_nonprofit: z.enum(['true', 'false']).optional(),

    // Fuzzy text search
    location: z.string().optional(),

    // Pagination
    limit: z.coerce.number().int().min(1).max(100).default(20),
    offset: z.coerce.number().int().min(0).default(0),
});

export type SearchInput = z.infer<typeof searchInputSchema>;

export const searchResponseSchema = z.object({
    data: z.array(z.any()),
    total: z.number(),
    limit: z.number(),
    offset: z.number(),
    query_time_ms: z.number(),
});

export type SearchResponse = z.infer<typeof searchResponseSchema>;
```

#### `src/lib/search/embeddings/generate.ts` (Uses Provider Abstraction)

```typescript
import { getEmbeddingProvider } from '../../embeddings/providers';

// Default embedding config (can be overridden)
const embeddingConfig = {
  provider: 'openai' as const,
  dimensions: 768,
};

// Get provider instance (singleton pattern)
const embeddingProvider = getEmbeddingProvider(embeddingConfig);

/**
 * Generate embedding for search query
 * Uses provider abstraction - easy to swap vendors
 */
export async function generateEmbedding(text: string): Promise<number[]> {
  return embeddingProvider.generate(text);
}

/**
 * Generate embeddings in batch
 */
export async function generateEmbeddingsBatch(
  texts: string[]
): Promise<number[][]> {
  return embeddingProvider.generateBatch(texts);
}
```

#### `src/lib/search/query.ts`

```typescript
import { neon } from '@neondatabase/serverless';
import { generateEmbedding } from './embeddings/generate';
import { buildFilterClauses, buildWhereClause } from './filters/build';
import { SEARCH_SCORING, HNSW_CONFIG } from './scoring/weights';
import type { ParsedFilters } from './filters/parse';

const sql = neon(process.env.DATABASE_URL!);

export interface SearchParams {
    query: string;
    filters: ParsedFilters;
    limit: number;
    offset: number;
}

export interface SearchResult {
    id: string;
    name: string;
    slug: string | null;
    website: string | null;
    logo_url: string | null;
    one_liner: string | null;
    tags: string[];
    industries: string[];
    regions: string[];
    batch: string | null;
    team_size: number | null;
    all_locations: string | null;
    is_hiring: boolean;
    stage: string | null;
    semantic_score: number;
    name_score: number;
    text_score: number;
    final_score: number;
}

/**
 * Main search function - orchestrates embedding generation and query execution
 * This is the only impure function - all helpers are pure
 *
 * Follows async-parallel pattern from react-best-practices:
 * - Start promise for embedding generation early
 * - Build filter clauses while embedding is generating
 * - Await embedding only when needed for query
 */
export async function searchCompanies(
    params: SearchParams
): Promise<SearchResult[]> {
    const { query, filters, limit, offset } = params;

    // Start embedding generation early (async-parallel pattern)
    const embeddingPromise = generateEmbedding(query);

    // Build filter clauses while embedding generates (parallel work)
    const filterClauses = buildFilterClauses(sql, filters);
    const whereClause = buildWhereClause(sql, filterClauses);

    // Await embedding only when needed
    const queryEmbedding = await embeddingPromise;

    // Set HNSW parameters for accuracy
    await sql`SET hnsw.ef_search = ${HNSW_CONFIG.EF_SEARCH}`;

    // Execute semantic search with hybrid scoring
    const results = await sql<SearchResult[]>`
    SELECT 
      id, name, slug, website, logo_url, one_liner,
      tags, industries, regions, batch, team_size,
      all_locations, is_hiring, stage,
      
      -- Semantic similarity score (0-1, higher = better)
      (1 - (embedding <=> ${JSON.stringify(queryEmbedding)}::vector)) AS semantic_score,
      
      -- Name fuzzy match score (0-1)
      similarity(name, ${query}) AS name_score,
      
      -- Full-text search score
      ts_rank_cd(search_vector, plainto_tsquery('english', ${query})) AS text_score,
      
      -- Weighted final score (configured for accuracy)
      (
        (1 - (embedding <=> ${JSON.stringify(queryEmbedding)}::vector)) * ${SEARCH_SCORING.SEMANTIC_WEIGHT} + 
        similarity(name, ${query}) * ${SEARCH_SCORING.NAME_WEIGHT} +
        ts_rank_cd(search_vector, plainto_tsquery('english', ${query})) * ${SEARCH_SCORING.FULLTEXT_WEIGHT}
      ) AS final_score
      
    FROM companies
    ${whereClause}
    ORDER BY final_score DESC
    LIMIT ${limit}
    OFFSET ${offset}
  `;

    return results;
}

/**
 * Get total count for pagination
 * Uses async-parallel pattern: start embedding early, build filters while waiting
 */
export async function getSearchCount(
    query: string,
    filters: ParsedFilters
): Promise<number> {
    // Start embedding generation early
    const embeddingPromise = generateEmbedding(query);

    // Build clauses in parallel
    const filterClauses = buildFilterClauses(sql, filters);
    const whereClause = buildWhereClause(sql, filterClauses);

    // Await embedding
    const queryEmbedding = await embeddingPromise;

    const result = await sql`
    SELECT COUNT(*)::int as count
    FROM companies
    ${whereClause}
    AND (1 - (embedding <=> ${JSON.stringify(queryEmbedding)}::vector)) >= ${SEARCH_SCORING.MIN_SEMANTIC_SCORE}
  `;

    return result[0].count;
}
```

#### `src/app/api/search/route.ts`

```typescript
import { type NextRequest, NextResponse } from 'next/server';
import { searchCompanies, getSearchCount } from '@/lib/search/query';
import { parseSearchFilters } from '@/lib/search/filters/parse';
import { searchInputSchema } from '@/lib/validations/search.schema';

/**
 * Search API Route Handler
 *
 * Best practices applied:
 * - Edge runtime for low latency (Vercel edge network)
 * - async-parallel: Execute search and count in parallel with Promise.all
 * - Proper error handling with typed responses
 * - Input validation with Zod
 */
export const runtime = 'edge';

export async function GET(request: NextRequest) {
    const startTime = Date.now();

    try {
        // Extract and validate search params
        const { searchParams } = request.nextUrl;
        const rawParams = Object.fromEntries(searchParams);

        // Validate with Zod schema
        const validatedParams = searchInputSchema.parse(rawParams);

        // Parse filters (pure function - testable)
        const filters = parseSearchFilters(validatedParams);

        // Execute search and count in parallel (async-parallel pattern)
        const [results, total] = await Promise.all([
            searchCompanies({
                query: validatedParams.q,
                filters,
                limit: validatedParams.limit,
                offset: validatedParams.offset,
            }),
            getSearchCount(validatedParams.q, filters),
        ]);

        const queryTime = Date.now() - startTime;

        return NextResponse.json({
            data: results,
            total,
            limit: validatedParams.limit,
            offset: validatedParams.offset,
            query_time_ms: queryTime,
        });
    } catch (error) {
        console.error('Search error:', error);

        // Zod validation errors
        if (error instanceof Error && error.name === 'ZodError') {
            return NextResponse.json(
                {
                    error: 'Invalid search parameters',
                    details: error.message,
                },
                { status: 400 }
            );
        }

        // Generic errors
        return NextResponse.json(
            {
                error: 'Search failed',
                message:
                    error instanceof Error ? error.message : 'Unknown error',
            },
            { status: 500 }
        );
    }
}
```

**Success Criteria:**

- ‚úÖ API endpoint returns results for test queries
- ‚úÖ Filters work correctly (batch, tags, team_size, etc.)
- ‚úÖ Results are ranked by relevance
- ‚úÖ Query time < 200ms
- ‚úÖ Error handling works

---

### Phase 4: Testing & Validation (1 hour)

**Tasks:**

1. Test sample queries
2. Validate filter combinations
3. Test edge cases
4. Performance benchmarking

**Test Cases:**

```bash
# Test 1: Pure semantic query
curl "http://localhost:3000/api/search?q=open%20source%20postgres%20vector%20database%20in%20SF"

# Expected: Finds pgvector-related companies in San Francisco

# Test 2: Semantic + metadata filters
curl "http://localhost:3000/api/search?q=B2B%20infrastructure%20startups&batch=W24&team_size_max=20&location=remote"

# Expected: W24 B2B infra companies with small teams, remote-friendly

# Test 3: Semantic with tags
curl "http://localhost:3000/api/search?q=eval%20tooling%20for%20LLMs&tags=AI,ML"

# Expected: AI companies building LLM evaluation tools

# Test 4: Name search
curl "http://localhost:3000/api/search?q=Stripe"

# Expected: Stripe should be top result

# Test 5: Boolean filters
curl "http://localhost:3000/api/search?q=developer%20tools&is_hiring=true"

# Expected: Hiring developer tools companies

# Test 6: Pagination
curl "http://localhost:3000/api/search?q=AI%20startups&limit=10&offset=10"

# Expected: Second page of results
```

**Success Criteria:**

- ‚úÖ All test queries return relevant results
- ‚úÖ Filters work correctly in combination
- ‚úÖ Pagination works
- ‚úÖ No crashes or errors
- ‚úÖ Query time consistently < 200ms

---

## Code Structure

```
ycagent.ai/
‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îî‚îÄ‚îÄ migrations/
‚îÇ       ‚îú‚îÄ‚îÄ 001_create_companies.sql           # Existing
‚îÇ       ‚îî‚îÄ‚îÄ 002_add_vector_search.sql          # New ‚úÖ
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ verify-phase1.ts                       # New ‚úÖ - Phase 1 verification
‚îÇ   ‚îî‚îÄ‚îÄ generate-embeddings-bulk.ts            # New üöß - One-time embedding gen
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ search/
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ route.ts                   # New - API endpoint
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ lib/
‚îÇ       ‚îú‚îÄ‚îÄ embeddings/                        # New üöß - Vendor-agnostic design
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ index.ts                       # Public API (barrel export)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ types.ts                       # Interfaces (vendor-agnostic)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ text-preparation.ts            # Pure functions (vendor-agnostic)
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ providers/
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ index.ts                   # Factory & registry
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ openai.ts                  # OpenAI implementation
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ search/                            # New - Modular search architecture
‚îÇ           ‚îú‚îÄ‚îÄ embeddings/
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ generate.ts                # Uses provider abstraction
‚îÇ           ‚îú‚îÄ‚îÄ filters/
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ parse.ts                   # Pure: params ‚Üí typed filters
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ build.ts                   # Pure: filters ‚Üí SQL clauses
‚îÇ           ‚îú‚îÄ‚îÄ scoring/
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ weights.ts                 # Pure: scoring config & functions
‚îÇ           ‚îî‚îÄ‚îÄ query.ts                       # Impure: orchestration
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ validations/
‚îÇ           ‚îú‚îÄ‚îÄ company.schema.ts              # Existing
‚îÇ           ‚îî‚îÄ‚îÄ search.schema.ts               # New - Search validation
‚îÇ
‚îî‚îÄ‚îÄ .env
    ‚îú‚îÄ‚îÄ DATABASE_URL=...                       # Existing ‚úÖ
    ‚îî‚îÄ‚îÄ OPENAI_API_KEY=sk-...                  # Added ‚úÖ
```

### Architecture Benefits

- **Vendor Abstraction**: Easy to swap OpenAI for Cohere, Anthropic, or local models
- **Pure Functions**: `text-preparation.ts`, `parse.ts`, `build.ts`, `weights.ts` are all testable
- **Provider Pattern**: Add new embedding providers without touching existing code
- **Separation of Concerns**: Each module has single responsibility
- **Easy to Modify**: Change scoring weights or providers without touching SQL
- **Type Safety**: TypeScript types flow through the entire pipeline
- **Future-Proof**: Decoupled design allows for vendor migration or multi-provider fallback

---

## Testing Strategy

### Unit Tests

```typescript
// tests/lib/embeddings/generate.test.ts

describe('createEmbeddingText', () => {
    it('should create proper text from company data', () => {
        const company = {
            name: 'Stripe',
            one_liner: 'Payment infrastructure for the internet',
            tags: ['Fintech', 'API'],
            batch: 'S09',
        };

        const text = createEmbeddingText(company);
        expect(text).toContain('Stripe');
        expect(text).toContain('Payment infrastructure');
        expect(text).toContain('Fintech');
    });
});
```

### Integration Tests

```typescript
// tests/api/search.test.ts

describe('Search API', () => {
    it('should return results for semantic query', async () => {
        const response = await fetch('/api/search?q=AI companies');
        const data = await response.json();

        expect(response.status).toBe(200);
        expect(data.data).toBeInstanceOf(Array);
        expect(data.total).toBeGreaterThan(0);
    });

    it('should respect batch filter', async () => {
        const response = await fetch('/api/search?q=startups&batch=W24');
        const data = await response.json();

        data.data.forEach((company) => {
            expect(company.batch).toBe('W24');
        });
    });
});
```

### Manual Testing Checklist

- [ ] Test all sample queries from requirements
- [ ] Test each filter independently
- [ ] Test filter combinations
- [ ] Test pagination edge cases (first page, last page, beyond last)
- [ ] Test empty results
- [ ] Test invalid inputs
- [ ] Test rate limiting (if implemented)
- [ ] Test with production data

---

## Configuration for Accuracy

### HNSW Configuration

```sql
-- Optimize for accuracy over speed
SET hnsw.ef_search = 200;  -- Higher recall (default: 40)

-- Build-time optimization
SET maintenance_work_mem = '4GB';  -- Better index quality during creation
SET max_parallel_maintenance_workers = 4;  -- Faster index builds
```

### Scoring Configuration

All scoring weights are centralized in `src/lib/search/scoring/weights.ts`:

```typescript
export const SEARCH_SCORING = {
    SEMANTIC_WEIGHT: 0.8, // 80% - Prioritize semantic understanding
    NAME_WEIGHT: 0.15, // 15% - Name fuzzy matching
    FULLTEXT_WEIGHT: 0.05, // 5% - Keyword fallback

    MIN_SEMANTIC_SCORE: 0.1,
    MIN_NAME_SCORE: 0.3,
};
```

**Tuning Guide:**

- Increase `SEMANTIC_WEIGHT` if users search by concepts ("eval tools")
- Increase `NAME_WEIGHT` if users search by company names ("Stripe")
- Adjust thresholds based on production feedback

### Expected Performance

| Metric                   | Target    | Notes                             |
| ------------------------ | --------- | --------------------------------- |
| **Query Latency**        | 150-250ms | OpenAI API: ~100ms, DB: ~50-100ms |
| **Accuracy (Recall@20)** | > 95%     | High ef_search ensures accuracy   |
| **Index Size**           | ~200MB    | For 10k companies                 |

---

## Cost Analysis

### One-Time Setup Costs

| Item                     | Quantity        | Unit Cost       | Total      |
| ------------------------ | --------------- | --------------- | ---------- |
| **Embedding Generation** | 5,653 companies | $0.13/1M tokens | **$1-1.5** |
| **Index Creation**       | One-time        | $0              | **$0**     |
| **Development Time**     | ~6 hours        | -               | -          |
| **Total Setup**          | -               | -               | **~$1.5**  |

**Note:** Cost reduced due to 768 dimensions vs 1536 (50% fewer dimensions, ~30% cost reduction per embedding due to lower token count)

### Ongoing Costs

#### Per Query

| Component           | Cost         | Notes                    |
| ------------------- | ------------ | ------------------------ |
| OpenAI Embedding    | $0.0001      | text-embedding-3-small   |
| Neon Compute        | ~$0.00001    | Billed by compute time   |
| Neon Storage        | $0           | 200MB << 0.5GB free tier |
| **Total per query** | **~$0.0001** | Essentially OpenAI cost  |

#### Monthly Estimates

| Usage Level | Queries/Month | Cost  |
| ----------- | ------------- | ----- |
| **Low**     | 10,000        | $0.70 |
| **Medium**  | 100,000       | $7    |
| **High**    | 1,000,000     | $70   |

**Note:** These are conservative estimates. Actual costs may be lower with caching. Costs reduced ~30% due to 768 dimensions vs 1536.

---

## Deployment Plan

### Stage 1: Development (Current)

**Tasks:**

- [x] Architecture planning
- [x] Database migration (Phase 1 ‚úÖ)
- [x] Vendor-agnostic embedding architecture design
- [ ] Embedding generation (Phase 2 üöß)
- [ ] Search implementation (Phase 3)
- [ ] Local testing (Phase 4)

**Duration:** 1-2 days  
**Risk:** Low  
**Progress:** Phase 1 Complete, Phase 2 In Progress

---

### Stage 2: Production Deployment (Greenfield)

**Tasks:**

- [ ] Deploy database migration to Neon
- [ ] Run embedding generation (one-time)
- [ ] Deploy search API to Vercel
- [ ] Integration testing
- [ ] Monitor initial usage

**Duration:** 1 day  
**Risk:** Low (no existing search to break)

**Deployment Steps:**

1. Run migration during off-hours
2. Generate embeddings (takes 1-2 hours)
3. Deploy API endpoint
4. Test with sample queries
5. Monitor for 24 hours

**Note:** No feature flags needed - this is a greenfield feature.

---

### Stage 3: Monitoring & Iteration

**Metrics to Track:**

1. **Search Quality** (Primary Focus)
    - Click-through rate on results
    - Zero-result queries
    - Result relevance (manual spot checks)

2. **Query Performance**
    - P50, P95, P99 latency
    - Error rate

3. **Cost Metrics**
    - OpenAI API costs
    - Neon compute costs

4. **Usage Patterns**
    - Most common queries
    - Most used filters
    - Query types (semantic vs name-based)

**Iteration Strategy:**

1. Collect 1 week of usage data
2. Analyze query patterns and quality
3. Tune scoring weights if needed
4. Adjust `hnsw.ef_search` based on accuracy needs
5. Optimize only if performance becomes an issue

---

## Troubleshooting Guide

### If Search Quality Is Poor

**Symptoms:**

- Irrelevant results in top 10
- Good companies not appearing
- Users report bad matches

**Diagnosis:**

1. Check sample queries manually
2. Look at semantic_score distribution
3. Review scoring weights

**Fixes:**

1. Increase `hnsw.ef_search` to 300-400 (better recall)
2. Adjust scoring weights in `weights.ts`
3. Regenerate embeddings if data has changed significantly

### If Queries Are Slow

**Symptoms:**

- Latency > 300ms consistently
- Timeouts

**Diagnosis:**

1. Check OpenAI API latency
2. Check database query time
3. Look at `hnsw.ef_search` value

**Fixes:**

1. Decrease `hnsw.ef_search` to 100 (faster, slightly less accurate)
2. Add caching for common queries
3. Check Neon compute size

### If Costs Are High

**Symptoms:**

- OpenAI bill > $100/month
- Many duplicate embeddings generated

**Diagnosis:**

1. Check query volume
2. Look for duplicate/similar queries

**Fixes:**

1. Cache embeddings for common queries
2. Deduplicate similar queries (fuzzy match)
3. Add rate limiting per user

---

## Future Enhancements

### Post-Launch Optimizations

**Only implement after collecting real usage data:**

1. **Query Caching**
    - Cache embeddings for frequent queries
    - Redis/Vercel KV integration
    - Implement only if costs justify it

2. **Query Understanding**
    - Parse natural language filters
    - Example: "W24 startups" ‚Üí auto-set batch filter
    - Build after seeing common query patterns

3. **Result Explanation**
    - Show why each result matched
    - Highlight relevant text
    - Improves user trust

4. **Advanced Filters**
    - Date ranges (founded_at)
    - Multiple batch selection
    - Add based on user requests

5. **Search Analytics Dashboard**
    - Popular searches
    - Zero-result queries
    - Quality metrics over time

### Performance Optimizations (If Needed)

**Implement only if performance becomes an issue:**

1. **Tiered Search**
    - Metadata-only for simple queries
    - Semantic for complex queries
    - Adds complexity, only if justified

2. **Connection Pooling**
    - Pool database connections
    - Reduces connection overhead

3. **Embedding Caching**
    - Cache in Redis/KV store
    - Reduces OpenAI API calls

---

## Appendix

## Architecture Decisions & Rationale

### Decoupled Embedding Architecture

**Decision:** Implement vendor-agnostic embedding layer with provider abstraction

**Rationale:**

1. **Future-Proofing**: Easy to migrate from OpenAI to alternatives (Cohere, Anthropic, local models)
2. **Cost Optimization**: Can switch to cheaper providers without code changes
3. **Vendor Lock-in Mitigation**: Not dependent on single provider
4. **Multi-Provider Fallback**: Can implement fallback chains (OpenAI ‚Üí Cohere ‚Üí Local)
5. **Testing**: Mock providers easily for unit tests

**Implementation Strategy:**

- **Pure functions** for text preparation (no vendor dependencies)
- **Provider interface** for abstraction
- **Factory pattern** for provider instantiation
- **Tight coupling acceptable** in one-time scripts

**Migration Path:**

To switch from OpenAI to Cohere (example):

```typescript
// Before (OpenAI)
const provider = getEmbeddingProvider({
  provider: 'openai',
  dimensions: 768,
});

// After (Cohere)
const provider = getEmbeddingProvider({
  provider: 'cohere',
  dimensions: 768,
});
```

Zero changes needed in:
- Text preparation logic
- Database queries
- Search implementation
- API routes

---

### Dimension Reduction (1536 ‚Üí 768)

**Decision:** Use 768 dimensions instead of 1536

**Benefits:**

- **50% storage reduction**: ~57-79MB instead of ~190-260MB
- **30% cost reduction**: ~$1.50 instead of ~$3 for one-time setup
- **Faster queries**: Smaller vectors = faster cosine similarity
- **Sufficient quality**: 768 dimensions still capture semantic meaning well

**Trade-offs:**

- Slightly lower accuracy (negligible for most queries)
- Can increase dimensions later if needed (requires re-generation)

---

### Sample Queries & Expected Results

**Query 1:** "open source postgres vector database in SF"

**Expected Results:**

- Companies building Postgres tools
- Vector database companies
- Open source database projects
- Location: San Francisco

**Query 2:** "W24 B2B infra startups, small team, remote"

**Filters Extracted:**

- batch=W24
- team_size_max=20
- location=remote

**Expected Results:**

- YC W24 batch companies
- Building B2B infrastructure
- Small teams (< 20 people)
- Remote-friendly

**Query 3:** "companies building eval tooling for LLMs"

**Expected Results:**

- LLM evaluation platforms
- Testing tools for AI models
- Quality assurance for LLMs
- AI observability tools

---

### Environment Variables

```bash
# .env.local

# Database
DATABASE_URL=postgresql://...neon.tech/main

# OpenAI
OPENAI_API_KEY=sk-...

# Optional: Redis for caching
UPSTASH_REDIS_URL=https://...
UPSTASH_REDIS_TOKEN=...
```

---

### Useful Commands

```bash
# Database operations
npm run migrate:run              # Run migrations
npm run migrate:rollback         # Rollback last migration

# Embedding generation
npx tsx scripts/generate-embeddings-bulk.ts

# Test search locally
npm run dev
curl "http://localhost:3000/api/search?q=test"

# Production deployment
vercel --prod
```

---

### References

- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Neon pgvector Guide](https://neon.com/docs/extensions/pgvector)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)
- [HNSW Algorithm](https://arxiv.org/abs/1603.09320)

---

## Implementation Status

### ‚úÖ Completed

- **Phase 1: Database Setup** (January 24, 2026)
  - pgvector extension enabled
  - Vector column added (768 dimensions)
  - HNSW index created (m=16, ef_construction=64)
  - Full-text search index created
  - Filter indexes created (batch, stage, team_size)
  - Data integrity verified (5,653 companies)

### üöß In Progress

- **Phase 2: Embedding Generation**
  - Decoupled architecture designed
  - Vendor-agnostic interfaces defined
  - Ready for implementation

### üìã Pending

- **Phase 3: Search Query Implementation**
- **Phase 4: Testing & Validation**

---

**Current Status:** Phase 1 Complete ‚úÖ | Phase 2 Ready üöß  
**Next Step:** Implement Phase 2 (Embedding Generation)  
**Command:** `npx dotenvx run -- tsx scripts/generate-embeddings-bulk.ts`
